# backend/news_ingestor.py
"""
News Ingestor (single-cycle)
- Reads NEWS_SOURCES (comma-separated URLs or JSON array).
- Pulls Alpaca news when configured via "alpaca:SYMB1|SYMB2|...".
- Optionally calls your local news_scraper.py if present, to merge custom items.
- Dedupe + atomic append to data/news_beliefs.csv.
- Writes backend/news_ingestor_metrics.json (atomic).
- Safe on errors; structured, timestamped stdout logs.
- Run mode: one cycle only (loop handled by news_ingestor_loop.py).
"""

import csv
import json
import os
import re
import sys
import tempfile
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

def _utc_now_iso() -> str:
    return datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

def _log(msg: str):
    ts = _utc_now_iso()
    print(f"[{ts}] [news_ingestor] {msg}")

# Optional deps â€” keep import failures non-fatal for dev
try:
    import requests
except Exception:  # pragma: no cover
    requests = None

try:
    import feedparser  # RSS/Atom
except Exception:  # pragma: no cover
    feedparser = None

# Optional custom scraper module (yours). If present, we can call it.
CUSTOM_SCRAPER_FUNCS = []
try:
    # Prefer absolute import when running from repo root
    from backend import news_scraper as custom_scraper  # type: ignore
    # Examine common function names
    for candidate in ("get_custom_news", "scrape_news", "get_news", "scrape"):
        if hasattr(custom_scraper, candidate) and callable(getattr(custom_scraper, candidate)):
            CUSTOM_SCRAPER_FUNCS.append(getattr(custom_scraper, candidate))
except Exception:
    # No custom scraper: that's okay
    custom_scraper = None  # type: ignore

BASE_DIR = Path(__file__).resolve().parent
# Canonical data dir at repo root (aligns with hot_trades_router.py)
REPO_ROOT = Path(__file__).resolve().parents[1]
DATA_DIR = REPO_ROOT / "data"
DATA_DIR.mkdir(parents=True, exist_ok=True)

OUT_CSV = DATA_DIR / "news_beliefs.csv"
METRICS_JSON = BASE_DIR / "news_ingestor_metrics.json"
STATE_JSON = BASE_DIR / "news_ingestor_state.json"  # persistent dedupe/last-run

print("[news_ingestor] INIT REPO_ROOT:", REPO_ROOT)
print("[news_ingestor] INIT NEWS_BELIEFS_CSV:", OUT_CSV)

ALPACA_DATA_BASE = os.getenv("ALPACA_DATA_BASE", "https://data.alpaca.markets")
ALPACA_API_KEY = os.getenv("ALPACA_API_KEY")
ALPACA_API_SECRET = os.getenv("ALPACA_API_SECRET")
print("[news_ingestor] INIT ALPACA_NEWS_ENABLED:", os.getenv("ALPACA_NEWS_ENABLED"))
print("[news_ingestor] INIT NEWS_SOURCES:", os.getenv("NEWS_SOURCES"))

def _csv_header() -> List[str]:
    return ["timestamp_utc", "story_id", "title", "url", "source", "summary", "tickers"]

def _ensure_csv():
    if not OUT_CSV.exists():
        with OUT_CSV.open("w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(_csv_header())

def _atomic_write_json(path: Path, data: Dict):
    tmp = path.with_suffix(path.suffix + ".tmp")
    tmp.write_text(json.dumps(data, indent=2))
    tmp.replace(path)

def _write_metrics(data: Dict):
    _atomic_write_json(METRICS_JSON, data)

def _atomic_write_text(path: Path, text: str):
    tmp = path.with_suffix(path.suffix + ".tmp")
    tmp.write_text(text)
    tmp.replace(path)

def _load_state() -> Dict:
    if STATE_JSON.exists():
        try:
            return json.loads(STATE_JSON.read_text())
        except Exception:
            pass
    return {"seen_ids": [], "last_run_utc": None}

def _save_state(state: Dict):
    _atomic_write_text(STATE_JSON, json.dumps(state, indent=2))

def _load_seen_ids() -> set:
    st = _load_state()
    return set(st.get("seen_ids", []))

def _remember_ids(new_ids: Iterable[str]):
    st = _load_state()
    seen = set(st.get("seen_ids", []))
    for i in new_ids:
        if i:
            seen.add(i)
    st["seen_ids"] = list(seen)[-100000:]  # cap memory
    st["last_run_utc"] = _utc_now_iso()
    _save_state(st)

def _parse_sources_env() -> List[str]:
    raw = os.getenv("NEWS_SOURCES", "").strip()
    if not raw:
        raw = "https://www.marketwatch.com/rss/topstories,https://feeds.a.dj.com/rss/RSSMarketsMain.xml"
    if not raw:
        return []
    # Allow JSON array or comma-separated
    if (raw.startswith("[") and raw.endswith("]")) or (raw.startswith("{") and raw.endswith("}")):
        try:
            obj = json.loads(raw)
            if isinstance(obj, list):
                return [str(x).strip() for x in obj if str(x).strip()]
            return []
        except Exception:
            return []
    # Comma-separated
    return [s.strip() for s in raw.split(",") if s.strip()]

# === Fetchers ===

def _normalize_text(s: Optional[str]) -> str:
    if not s:
        return ""
    s = re.sub(r"\s+", " ", s).strip()
    return s

def _fetch_rss(url: str, timeout: int = 10) -> List[Dict]:
    if feedparser is None:
        _log("WARNING: feedparser not installed; skipping RSS/Atom source.")
        return []
    try:
        parsed = feedparser.parse(url)
    except Exception as e:
        _log(f"ERROR fetching RSS {url}: {e}")
        return []
    out = []
    for e in parsed.entries:
        story_id = e.get("id") or e.get("guid") or e.get("link") or e.get("title")
        out.append({
            "story_id": str(story_id) if story_id else None,
            "title": e.get("title") or "",
            "url": e.get("link") or "",
            "source": url,
            "summary": e.get("summary") or e.get("description") or "",
            "tickers": [],  # optional enrichment later
        })
    return out

def _fetch_http_json(url: str, timeout: int = 10) -> List[Dict]:
    if requests is None:
        _log("WARNING: requests not installed; skipping JSON source.")
        return []
    try:
        resp = requests.get(url, timeout=timeout)
        resp.raise_for_status()
        data = resp.json()
    except Exception as e:
        _log(f"ERROR fetching JSON {url}: {e}")
        return []
    out = []
    if isinstance(data, dict) and "items" in data and isinstance(data["items"], list):
        items = data["items"]
    elif isinstance(data, list):
        items = data
    else:
        items = []
    for it in items:
        story_id = it.get("id") or it.get("uuid") or it.get("url")
        out.append({
            "story_id": str(story_id) if story_id else None,
            "title": it.get("title") or "",
            "url": it.get("url") or "",
            "source": url,
            "summary": _normalize_text(it.get("summary") or it.get("description") or ""),
            "tickers": it.get("tickers") or it.get("symbols") or [],
        })
    return out

def _fetch_alpaca_news(symbols: List[str], limit: int = 25) -> List[Dict]:
    if requests is None:
        _log("WARNING: requests not installed; skipping Alpaca.")
        return []
    if not ALPACA_API_KEY or not ALPACA_API_SECRET:
        _log("INFO: Alpaca credentials not set; skipping Alpaca news.")
        return []
    url = f"{ALPACA_DATA_BASE}/v1beta1/news"
    try:
        params = {"symbols": ",".join(symbols), "limit": str(limit)}
        headers = {
            "Apca-Api-Key-Id": ALPACA_API_KEY,
            "Apca-Api-Secret-Key": ALPACA_API_SECRET,
        }
        resp = requests.get(url, params=params, headers=headers, timeout=10)
        resp.raise_for_status()
        data = resp.json()
    except Exception as e:
        _log(f"ERROR fetching Alpaca news: {e}")
        return []
    items = data.get("news", []) if isinstance(data, dict) else []
    out = []
    for it in items:
        story_id = it.get("id") or it.get("uuid") or it.get("url")
        title = it.get("headline") or it.get("title") or ""
        url_item = it.get("url") or ""
        summary = it.get("summary") or ""
        syms = it.get("symbols") if isinstance(it.get("symbols"), list) else []
        out.append({
            "story_id": str(story_id) if story_id else None,
            "title": title,
            "url": url_item,
            "source": "alpaca",
            "summary": summary,
            "tickers": syms or [],
        })
    return out

def _infer_fetcher(entry: str) -> Tuple[str, Dict]:
    """
    Recognizes:
      - "alpaca:SYMB1|SYMB2|..." -> Alpaca news for symbols
      - http(s)://...           -> RSS/Atom (via feedparser) OR JSON (fallback)
      - feed://...              -> treat like RSS
    """
    if entry.startswith("alpaca:"):
        symbols = [s for s in entry.split(":", 1)[1].split("|") if s]
        return ("alpaca", {"symbols": symbols})
    if entry.startswith("feed://"):
        url = "http://" + entry[len("feed://"):]
        return ("rss", {"url": url})
    if entry.startswith("http://") or entry.startswith("https://"):
        # Try RSS via feedparser first, fall back to JSON fetch if looks JSON
        return ("rss", {"url": entry})
    return ("unknown", {})

def _collect_sources() -> List[Dict]:
    entries = _parse_sources_env()
    if not entries:
        _log("INFO: NEWS_SOURCES is empty and no custom scraper detected; nothing to ingest.")
        return []
    out = []
    for entry in entries:
        kind, params = _infer_fetcher(entry)
        if kind == "alpaca" and params.get("symbols"):
            out.append({"kind": "alpaca", "params": params})
        elif kind == "rss":
            out.append({"kind": "rss", "params": params})
        elif kind == "unknown" and entry:
            # Try JSON anyway
            out.append({"kind": "json", "params": {"url": entry}})
    return out

def _atomic_append_rows(rows: List[List[str]]):
    """Atomic append: read current -> write merged tmp -> replace."""
    _ensure_csv()
    with OUT_CSV.open("r", newline="", encoding="utf-8") as rf:
        existing = rf.read()

    with tempfile.NamedTemporaryFile("w", delete=False, newline="", encoding="utf-8") as tf:
        tf.write(existing)
        w = csv.writer(tf)
        for row in rows:
            w.writerow(row)
        tmp_path = Path(tf.name)

    tmp_path.replace(OUT_CSV)

def main() -> int:
    start_ts = datetime.now(timezone.utc).timestamp()

    sources = _collect_sources()
    if not sources:
        _write_metrics({
            "polled_feeds": 0,
            "fetched": 0,
            "new_written": 0,
            "duration_seconds": 0.0,
            "alpaca_enabled": bool(ALPACA_API_KEY and ALPACA_API_SECRET),
            "custom_scraper": bool(CUSTOM_SCRAPER_FUNCS),
        })
        return 0

    # Seed CSV header if needed
    _ensure_csv()

    collected: List[Dict] = []
    fetched = 0

    # RSS / JSON / Alpaca fetch
    for s in sources:
        if s["kind"] == "alpaca":
            syms = s["params"].get("symbols") or []
            items = _fetch_alpaca_news(syms)
            fetched += len(items)
            collected.extend(items)
        elif s["kind"] == "rss":
            url = s["params"]["url"]
            items = _fetch_rss(url)
            fetched += len(items)
            collected.extend(items)
        elif s["kind"] == "json":
            url = s["params"]["url"]
            items = _fetch_http_json(url)
            fetched += len(items)
            collected.extend(items)

    # Custom scraper (optional)
    if CUSTOM_SCRAPER_FUNCS:
        total_custom = 0
        for func in CUSTOM_SCRAPER_FUNCS:
            try:
                custom_items = func()  # must return iterable of dicts
            except TypeError:
                # if function expects no args but we passed none (or vice versa), attempt calling with no args
                custom_items = func()
            except Exception as e:
                _log(f"WARNING: custom scraper {func.__name__} failed: {e}")
                continue

            mapped = []
            for it in (custom_items or []):
                # Map arbitrary dicts into our schema (best-effort)
                story_id = it.get("story_id") or it.get("id") or it.get("uuid") or it.get("url") or it.get("title")
                mapped.append({
                    "story_id": str(story_id) if story_id else None,
                    "title": it.get("title", ""),
                    "url": it.get("url", ""),
                    "source": it.get("source", "custom_scraper"),
                    "summary": it.get("summary", "") or it.get("description", ""),
                    "tickers": it.get("tickers", []) or it.get("symbols", []),
                })
            total_custom += len(mapped)
            collected.extend(mapped)
        _log(f"Custom scraper contributed {total_custom} items")

    # normalize fields
    for it in collected:
        it["story_id"] = (it.get("story_id") or "").strip()
        it["title"] = _normalize_text(it.get("title"))
        it["url"] = (it.get("url") or "").strip()
        it["source"] = (it.get("source") or "").strip()
        it["summary"] = _normalize_text(it.get("summary"))
        syms = it.get("tickers") or []
        if isinstance(syms, str):
            syms = [s.strip().upper() for s in syms.split(",") if s.strip()]
        if isinstance(syms, list):
            syms = [str(s).strip().upper() for s in syms if str(s).strip()]
        it["tickers"] = syms

    seen = _load_seen_ids()
    new_rows: List[List[str]] = []
    newly_seen: List[str] = []

    for it in collected:
        sid = it.get("story_id") or it.get("url") or ""
        if not sid:
            continue
        if sid in seen:
            continue

        newly_seen.append(sid)
        row = [
            _utc_now_iso(),
            sid,
            it.get("title", ""),
            it.get("url", ""),
            it.get("source", ""),
            it.get("summary", ""),
            ";".join(it.get("tickers", [])),
        ]
        new_rows.append(row)

    new_count = len(new_rows)
    if new_rows:
        _atomic_append_rows(new_rows)
        _remember_ids(newly_seen)

    duration = datetime.now(timezone.utc).timestamp() - start_ts
    _write_metrics({
        "polled_feeds": len(sources),
        "fetched": fetched,
        "new_written": new_count,
        "duration_seconds": round(duration, 3),
        "alpaca_enabled": bool(ALPACA_API_KEY and ALPACA_API_SECRET),
        "custom_scraper": bool(CUSTOM_SCRAPER_FUNCS),
    })

    _log(f"Cycle complete: feeds={len(sources)} fetched={fetched} new={new_count} elapsed={duration:.2f}s")
    return 0

if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        _log("Interrupted via KeyboardInterrupt.")
        sys.exit(130)
